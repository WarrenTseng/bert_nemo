{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/title.JPG\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "from nemo.collections.nlp.data.datasets import BertTextClassificationDataset\n",
    "from nemo.collections.nlp.nm.data_layers.text_classification_datalayer import BertTextClassificationDataLayer\n",
    "from nemo.collections.nlp.nm.trainables import SequenceClassifier\n",
    "\n",
    "from nemo.backends.pytorch.common import CrossEntropyLossNM\n",
    "from nemo.utils.lr_policies import get_lr_policy\n",
    "from nemo.collections.nlp.callbacks.text_classification_callback import eval_iter_callback, eval_epochs_done_callback\n",
    "from nemo import logging\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/nemo_intro.JPG\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA NeMo is a Python toolkit for building, training, and fine-tuning GPU-accelerated conversational AI models using a simple interface.\n",
    "Source: https://developer.nvidia.com/nvidia-nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'BERT' # 'BERT-large', 'Megatron-BERT'\n",
    "\n",
    "path_pool = {'BERT':'./checkpoint/bert/',\n",
    "             'BERT-large':'./checkpoint/bert_large/',\n",
    "             'Megatron-BERT':'./checkpoint/megatron_bert/'}\n",
    "name_pool = {'BERT':'bert-base-uncased',\n",
    "             'BERT-large':'bert-large-uncased',\n",
    "             'Megatron-BERT':'megatron-bert-uncased'}\n",
    "\n",
    "PRETRAINED_MODEL_NAME = name_pool[ENCODER]\n",
    "PRETRAINED_MODEL_CHECKPOINT = path_pool[ENCODER]+'bert.pt'\n",
    "PRETRAINED_MODEL_CONFIG = path_pool[ENCODER]+'bert_config.json'\n",
    "TRAIN_DATA_PATH = './data/train.tsv'\n",
    "TEST_DATA_PATH = './data/test.tsv'\n",
    "AMP_OPTIMIZATION_LEVEL = 'O2'\n",
    "WORK_DIR = 'output/'\n",
    "\n",
    "MAX_SEQ_LEN = 256\n",
    "DROPOUT_RATE = .3\n",
    "# batch size for RTX2080Ti\n",
    "if ENCODER=='BERT':\n",
    "    BATCH_SIZE = 32\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "n_epochs = 3\n",
    "lr_warmup_proportion = 0.1\n",
    "lr = 3e-5\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/components.JPG\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nf = nemo.core.NeuralModuleFactory(log_dir=WORK_DIR,\n",
    "                                   create_tb_writer=True,\n",
    "                                   add_time_to_log_dir=False,\n",
    "                                   optimization_level=AMP_OPTIMIZATION_LEVEL)\n",
    "model = nemo_nlp.nm.trainables.get_pretrained_lm_model(\n",
    "    pretrained_model_name=PRETRAINED_MODEL_NAME,\n",
    "    config=PRETRAINED_MODEL_CONFIG,\n",
    "    checkpoint=PRETRAINED_MODEL_CHECKPOINT\n",
    ")\n",
    "tokenizer = nemo.collections.nlp.data.tokenizers.get_tokenizer(\n",
    "    tokenizer_name='nemobert',\n",
    "    pretrained_model_name=PRETRAINED_MODEL_NAME,\n",
    "    do_lower_case=True\n",
    ")\n",
    "classifier = SequenceClassifier(    \n",
    "    hidden_size=model.hidden_size,\n",
    "    num_classes=2,\n",
    "    dropout=DROPOUT_RATE,\n",
    "    num_layers=2,\n",
    "    log_softmax=False,\n",
    ")\n",
    "loss_func = CrossEntropyLossNM()\n",
    "train_data_layer = BertTextClassificationDataLayer(\n",
    "    tokenizer=tokenizer,\n",
    "    input_file=TRAIN_DATA_PATH,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    use_cache=True\n",
    ")\n",
    "eval_data_layer = BertTextClassificationDataLayer(\n",
    "    tokenizer=tokenizer,\n",
    "    input_file=TEST_DATA_PATH,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_weights, model.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/create_graph.JPG\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_layer()\n",
    "train_hidden_states = model(input_ids=train_data.input_ids, token_type_ids=train_data.input_type_ids, attention_mask=train_data.input_mask)\n",
    "train_logits = classifier(hidden_states=train_hidden_states)\n",
    "loss = loss_func(logits=train_logits, labels=train_data.labels)\n",
    "# If you're training on multiple GPUs, this should be\n",
    "# len(train_data_layer) // (batch_size * batches_per_step * num_gpus)\n",
    "train_steps_per_epoch = len(train_data_layer) // BATCH_SIZE\n",
    "\n",
    "eval_data = eval_data_layer()\n",
    "eval_hidden_states = model(input_ids=eval_data.input_ids, token_type_ids=eval_data.input_type_ids, attention_mask=eval_data.input_mask)\n",
    "eval_logits = classifier(hidden_states=eval_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callback = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensors=[loss],\n",
    "    print_func=lambda x: logging.info(\"Loss: {:.3f}\".format(x[0].item())),\n",
    "    get_tb_values=lambda x: [[\"loss\", x[0]]],\n",
    "    step_freq=train_steps_per_epoch,\n",
    "    tb_writer=nf.tb_writer,\n",
    ")\n",
    "eval_callback = nemo.core.EvaluatorCallback(\n",
    "        eval_tensors=[eval_logits, eval_data.labels],\n",
    "        user_iter_callback=lambda x, y: eval_iter_callback(x, y, eval_data_layer),\n",
    "        user_epochs_done_callback=lambda x: eval_epochs_done_callback(x, f'{nf.work_dir}/graphs'),\n",
    "        tb_writer=nf.tb_writer,\n",
    "        eval_step=train_steps_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_policy_fn = get_lr_policy(\"WarmupAnnealing\", total_steps=n_epochs * train_steps_per_epoch, warmup_ratio=lr_warmup_proportion\n",
    ")\n",
    "nf.train(\n",
    "    tensors_to_optimize=[loss],\n",
    "    callbacks=[train_callback, eval_callback],\n",
    "    lr_policy=lr_policy_fn,\n",
    "    optimizer=\"adam_w\",\n",
    "    optimization_params={\"num_epochs\": n_epochs, \"lr\": lr, \"weight_decay\": weight_decay},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_to('model.nemo')\n",
    "classifier.save_to('classifier.nemo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
